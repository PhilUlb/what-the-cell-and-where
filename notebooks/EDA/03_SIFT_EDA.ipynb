{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220ea576-551b-483a-a504-1acdf4c4f49c",
   "metadata": {},
   "source": [
    "# EDA using SIFT\n",
    "\n",
    "SIFT identifies keypoints that are distinctive across an image’s width, height, and most importantly, scale. By considering scale, we can identify keypoints that will remain stable (to an extent) even when the template of interest changes size, when the image quality becomes better or worse, or when the template undergoes changes in viewpoint or aspect ratio. Moreover, each keypoint has an associated orientation that makes SIFT features invariant to template rotations. Finally, SIFT will generate a descriptor for each keypoint, a 128-length vector that allows keypoints to be compared. These descriptors are nothing more than a histogram of gradients computed within the keypoint’s neighborhood.\n",
    "\n",
    "# Let's have a look at some images and their Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e741448-b8e8-4577-9d2c-389d582373f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import timeit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62181d7-eebc-4eb9-bd48-1a0abd53bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = pathlib.Path(curr_dir).parents[1]\n",
    "filename = f\"{parent_dir}/data/data_original/train.csv\"\n",
    "data_dir = f\"{parent_dir}/data/data_original/train\"\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "df_cell_compressed = df.drop_duplicates(subset=['id'])\n",
    "df_cell_compressed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe0bc-2ddc-4e95-87fc-2e7694916fbd",
   "metadata": {},
   "source": [
    "# celltype shsy5y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec5247-59c4-40ca-8bb7-3b169390e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../../data/data_original/train/0030fd0e6378.png'\n",
    "img1 = cv2.imread(img_path)\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "#keypoints\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "imag_1 = cv2.drawKeypoints(gray1,keypoints_1,img1)\n",
    "\n",
    "# plots\n",
    "fig, arr = plt.subplots(1,2, figsize=(30, 30))\n",
    "arr[0].imshow(cv2.imread(img_path))\n",
    "arr[1].imshow(cv2.drawKeypoints(gray1,keypoints_1,img1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb47ac-7051-43ca-92f3-b0441b883fd9",
   "metadata": {},
   "source": [
    "# celltype astrocytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4598f-fab9-4f80-9a7b-5f2cdf5220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nOctaveLayers = 42\n",
    "\n",
    "nfeatures=500\n",
    "contrastThreshold = 0.05\n",
    "edgeThreshold = 0.025\n",
    "sigma = 0.98\n",
    "\n",
    "img_path = '../../data/data_original/train/0140b3c8f445.png'\n",
    "img1 = cv2.imread(img_path)\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "#keypoints\n",
    "sift = cv2.xfeatures2d.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold,edgeThreshold, sigma)\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "imag_1 = cv2.drawKeypoints(gray1,keypoints_1,img1)\n",
    "\n",
    "# plots\n",
    "fig, arr = plt.subplots(1,2, figsize=(30, 30))\n",
    "arr[0].imshow(cv2.imread(img_path))\n",
    "arr[1].imshow(cv2.drawKeypoints(gray1,keypoints_1,img1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55cc26-738f-43a8-a53d-7720d7d1a7d2",
   "metadata": {},
   "source": [
    "# celltype cort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a8546-e343-4fc9-9d21-952755a2709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../../data/data_original/train/01ae5a43a2ab.png'\n",
    "img1 = cv2.imread(img_path)\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "#keypoints\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "imag_1 = cv2.drawKeypoints(gray1,keypoints_1,img1)\n",
    "\n",
    "# plots\n",
    "fig, arr = plt.subplots(1,2, figsize=(30, 30))\n",
    "arr[0].imshow(cv2.imread(img_path))\n",
    "arr[1].imshow(cv2.drawKeypoints(gray1,keypoints_1,img1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeb669-2774-47d0-8819-f5718ceaaa7d",
   "metadata": {},
   "source": [
    "# Feature Matching comparing images of different cell types\n",
    "\n",
    "* we can compare images of different cell types and have a look if those images have the same keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b8428-ad6b-42c7-a61d-b307116f5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../../data/data_original/train/085eb8fec206.png' #astro\n",
    "#img_path = '../../data/data_original/train/01ae5a43a2ab.png' #cort\n",
    "\n",
    "img1 = cv2.imread(img_path)\n",
    "\n",
    "#img_path2 = '../../data/data_original/train/01ae5a43a2ab.png' #same cort\n",
    "#img_path2 = '../../data/data_original/train/026b3c2c4b32.png' #cort\n",
    "img_path2 = '../../data/data_original/train/0030fd0e6378.png'#shsy5y\n",
    "#img_path2 = '../../data/data_original/train/0140b3c8f445.png' #astrocytes\n",
    "img2 = cv2.imread(img_path2)\n",
    "\n",
    "\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "#sift\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n",
    "#feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "matches = bf.match(descriptors_1,descriptors_2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:10], img2, flags=2)\n",
    "fig= plt.figure(figsize=(30,60))\n",
    "plt.imshow(img3),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0015dc-fbc1-43a5-be28-45f116ec137b",
   "metadata": {},
   "source": [
    "# Feature matching loop\n",
    "\n",
    "* we go a step further and search for keypoint matches by looping over different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d4e65-6411-43f6-961f-c7ad858e7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../../data/data_original/train/085eb8fec206.png' #astro\n",
    "#img_path = '../../data/data_original/train/01ae5a43a2ab.png' #cort\n",
    "\n",
    "img1 = cv2.imread(img_path)\n",
    "\n",
    "#img_path2 = '../../data/data_original/train/01ae5a43a2ab.png' #same cort\n",
    "#img_path2 = '../../data/data_original/train/026b3c2c4b32.png' #cort\n",
    "img_path2 = '../../data/data_original/train/0030fd0e6378.png'#shsy5y\n",
    "#img_path2 = '../../data/data_original/train/0140b3c8f445.png' #astrocytes\n",
    "\n",
    "\n",
    "\n",
    "img1 = cv2.imread(img_path)\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "\n",
    "match_lst = []\n",
    "id_lst = []\n",
    "\n",
    "img_path2 = '../../data/data_original/train/'\n",
    "for item in os.listdir(img_path2):\n",
    "    try:\n",
    "        imgi = cv2.imread(img_path2 + item)\n",
    "        imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2GRAY)\n",
    "        #sift\n",
    "        keypoints_i, descriptors_i= sift.detectAndCompute(imgi,None)\n",
    "        #feature matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "        matches = bf.match(descriptors_1,descriptors_i)\n",
    "        matches = sorted(matches, key = lambda x:x.distance)\n",
    "        match_lst.append(len(matches))\n",
    "        id_lst.append(item)\n",
    "    except:\n",
    "        print(item)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c6c46-c602-4217-b8e0-745eaef67417",
   "metadata": {},
   "source": [
    "# Playing with parameters of SIFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8050767-4f74-44fa-8b16-eb5c4a590328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nOctaveLayers = 30\n",
    "nfeatures=2000\n",
    "contrastThreshold = 0.1\n",
    "edgeThreshold = 0.2\n",
    "sigma = 0.88\n",
    "\n",
    "#img_path = '../../data/data_original/train/0140b3c8f445.png'\n",
    "img_path = '../../data/data_original/train/085eb8fec206.png' #astro\n",
    "img1 = cv2.imread(img_path)\n",
    "\n",
    "#img_path2 = '../../data/data_original/train/01ae5a43a2ab.png' #same cort\n",
    "#img_path2 = '../../data/data_original/train/026b3c2c4b32.png' #cort\n",
    "#img_path2 = '../../data/data_original/train/0030fd0e6378.png'#shsy5y\n",
    "img_path2 = '../../data/data_original/train/0140b3c8f445.png' #astrocytes\n",
    "img2 = cv2.imread(img_path2)\n",
    "\n",
    "\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "#sift\n",
    "sift = cv2.xfeatures2d.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold,edgeThreshold, sigma)\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n",
    "#feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "matches = bf.match(descriptors_1,descriptors_2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:500], img2, flags=2)\n",
    "fig= plt.figure(figsize=(30,60))\n",
    "plt.imshow(img3),plt.show()\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbad0b7-50a1-40b5-af83-434e353d1ce5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification with SIFT feature matching\n",
    "\n",
    "As you can see in the dataframe above comparing the number of keypoint matches (NoKM) for each image pair, NoKM could be useful for classicication (Spoiler: Indeed, it turns out to be that case)\n",
    "\n",
    "### 1. First, we calculate keypoints / descriptors for each img as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8fc27-f27a-41a3-91ad-04b3afea59bc",
   "metadata": {},
   "source": [
    "* nice to know: sift.detect finds keypoints, sift.compute computes the descriptors from the keypoint we have found. the command used here (sift.detectAndCompute), does both at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d775fb-2fd5-4387-8351-a22b3d921040",
   "metadata": {},
   "source": [
    "### 1.1 Train and test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613a943-3e9a-43d9-94ba-1e0f95ed7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed seed for reproducability\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a5146-e5aa-427c-a215-6e8e0c2baa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_compressed2 = df_cell_compressed[['id','cell_type']]\n",
    "\n",
    "id_temp = []\n",
    "for i in range(0,df_cell_compressed2.shape[0]):\n",
    "    id_temp.append(\"../../data/data_original/train/\" + df_cell_compressed2.id.iloc[i] + \".png\")\n",
    "\n",
    "df_cell_compressed2['id'] = id_temp\n",
    "\n",
    "# 1. We reserve 20% = ~516 images of our data as test data\n",
    "train, test, y_train, y_test = train_test_split(df_cell_compressed2, \n",
    "                                                df_cell_compressed2.cell_type, \n",
    "                                                test_size=0.3, random_state=RSEED)\n",
    "\n",
    "train.to_csv('cells_train.csv', header = False, index = False)\n",
    "test.to_csv('cells_test.csv', header = False, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0956ae-f42d-44c3-b025-631f4a256182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('cells_train.csv', names=[\"img_path\", \"cell_type\"])\n",
    "df_test = pd.read_csv('cells_test.csv', names=[\"img_path\", \"cell_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8534fd-2d75-4ff3-b4de-32a9d7c3510c",
   "metadata": {},
   "source": [
    "### 2.1 keypoint detection for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f2aa2-7f7a-49dd-bd5c-a788729483a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nOctaveLayers = 42\n",
    "\n",
    "nfeatures=500\n",
    "contrastThreshold = 0.05\n",
    "edgeThreshold = 0.1\n",
    "sigma = 1\n",
    "\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma)\n",
    "\n",
    "train_keypoint_lst = []\n",
    "train_descriptor_lst = []\n",
    "train_img_lst = []\n",
    "train_cell_lst = []\n",
    "for i in range(0,df_train.shape[0]):  #key_train_df.shape[0]  \n",
    "        imgi = cv2.imread(df_train.img_path[i]) \n",
    "        imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2GRAY)\n",
    "        #sift\n",
    "        keypoints_i, descriptors_i= sift.detectAndCompute(imgi,None)\n",
    "        #feature matching\n",
    "        train_img_lst.append(df_train.img_path[i])\n",
    "        train_cell_lst.append(df_train.cell_type[i])\n",
    "        train_keypoint_lst.append(keypoints_i)\n",
    "        train_descriptor_lst.append(descriptors_i)\n",
    "key_train_df = pd.DataFrame()\n",
    "key_train_df['img'] = train_img_lst\n",
    "key_train_df['cell_type'] = train_cell_lst\n",
    "key_train_df['keypoints'] = train_keypoint_lst\n",
    "key_train_df['descriptors'] = train_descriptor_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f312228-a8fc-4f60-976a-cfe0e6fd9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45692eac-7c18-4735-bde5-20722426c9b2",
   "metadata": {},
   "source": [
    "## 2.2 keypoint detection for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a921025-aa0d-4038-9efc-1c17698a6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keypoint_lst = []\n",
    "test_descriptor_lst = []\n",
    "test_img_lst = []\n",
    "test_cell_lst = []\n",
    "for i in range(0,df_test.shape[0]):  #key_test_df.shape[0]         \n",
    "        imgi = cv2.imread(df_test.img_path[i]) \n",
    "        imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2GRAY)\n",
    "        #sift\n",
    "        keypoints_i, descriptors_i= sift.detectAndCompute(imgi,None)\n",
    "        #feature matching\n",
    "        test_img_lst.append(df_test.img_path[i])\n",
    "        test_cell_lst.append(df_test.cell_type[i])\n",
    "        test_keypoint_lst.append(keypoints_i)\n",
    "        test_descriptor_lst.append(descriptors_i)\n",
    "key_test_df = pd.DataFrame()\n",
    "key_test_df['img'] = test_img_lst\n",
    "key_test_df['cell_type'] = test_cell_lst\n",
    "key_test_df['keypoints'] = test_keypoint_lst\n",
    "key_test_df['descriptors'] = test_descriptor_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d23f7-a5f6-4b8c-90ea-9f12f240aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee80ea-f999-4e2d-99a2-56ec9069747f",
   "metadata": {},
   "source": [
    "3. Calculating NoKM for each image from the test test with every image of the train set\n",
    "\n",
    "* How do we use that to classify:\n",
    "Let's say, image \"x\" (from test) has 200 keypoints. We check each image from train set wether it has the same keypoints, count the numbers of those keypoints (keypoint matches) und put it into a list. After iterating through all images in the train set, we sort the list with the highest NoKM on top of a dataframe, in which the corresponding cell type of the train image is also saved.\n",
    "\n",
    "* Now, we do a voting with the first 30 images. The cell type which occurs the most, defines the label of the test image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7e6ea-73db-43b9-b6e5-bc4c38089a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_actual = []\n",
    "j_start = 0\n",
    "j_end = key_test_df.shape[0] \n",
    "\n",
    "for j in range(j_start, j_end): #key_train_df.shape[0] \n",
    "    match_lst = []\n",
    "    cell_lst = []\n",
    "    keypoints_j = key_test_df.keypoints[j]\n",
    "    descriptors_j = key_test_df.descriptors[j]\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    for i in range(0, key_train_df.shape[0]):  #df_train.shape[0]         \n",
    "        #sift\n",
    "        keypoints_i =  key_train_df.keypoints[i]\n",
    "        descriptors_i= key_train_df.descriptors[i]\n",
    "        #feature matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "        matches = bf.match(descriptors_j,descriptors_i)\n",
    "        matches = sorted(matches, key = lambda x:x.distance)\n",
    "        match_lst.append(len(matches))\n",
    "        cell_lst.append(df_train.cell_type[i])\n",
    "    #create temporary dataframe\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['keypoint_matches'] = match_lst\n",
    "    new_df['cell_type'] = cell_lst\n",
    "    new_df = new_df.sort_values('keypoint_matches', ascending = False).head(29)\n",
    "    y_pred.append(list(new_df.cell_type.mode()))\n",
    "    y_actual.append(df_test.cell_type[j])\n",
    "    print(j+1, 'von', j_end+1)\n",
    "    print('actual cell_type:', df_test.cell_type[j])\n",
    "    print('predicted cell_type:',list(new_df.cell_type.mode()))\n",
    "    #time measurement\n",
    "    stop = timeit.default_timer()\n",
    "    print('This prediction took', stop - start, 'seconds')\n",
    "    print('---'*10)  \n",
    "    del new_df\n",
    "\n",
    "sum_df = pd.DataFrame()\n",
    "sum_df['actual'] = y_actual\n",
    "sum_df['predicted'] = y_pred\n",
    "sum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b31429-da2b-4eec-846b-c4165c346479",
   "metadata": {},
   "source": [
    "## 3. corresponding Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0a27e-dc88-4ac3-861b-98fa95533a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum_df.drop(sum_df.index[[80, 129]]) #we drop them, because voting is not distinct\n",
    "y_test = list(a.actual)\n",
    "y_pred = list(a.predicted)\n",
    "cfm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cfm, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5);\n",
    "\n",
    "report_dt = classification_report(y_test, y_pred)\n",
    "print(report_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8204374-39d8-444e-be68-0a16a2c03d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('summary_sift_classification.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f175e38-2ac6-4edb-b875-034c2d5645c2",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The classification of this (very straight forward) method is surprisingly accurate (0.89). Unfortunately, we misclassify 54% of the astrocytes as shsy5y. This seems to be a limitation of this method when it comes to this microscopic images. Other SIFT parameters lead to worse results.\n",
    "The drawback of this method is the running time of the algorithm, escpecially the calculations of the keypoints takes several minutes on just 600 images. Therefore, we skip using this method for an increased dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622528f0-270b-4aed-a366-bf42f66611a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
